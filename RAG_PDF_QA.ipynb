{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNG9XvnHQQVRXG+s3xX557",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdurrahmanrussel/rag-pdf-qa/blob/main/RAG_PDF_QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrxtQx6HIN8W"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install --quiet langchain sentence-transformers faiss-cpu PyPDF2 pytesseract pdf2image transformers peft trl bitsandbytes\n",
        "!apt-get install -y -qq tesseract-ocr poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Hugging Face token\n",
        "\n",
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "hf_token = getpass(\"Enter your Hugging Face token: \")\n",
        "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = hf_token"
      ],
      "metadata": {
        "id": "yEJMTQNfIRmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Upload PDFs\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "uploaded_files = files.upload()\n",
        "pdf_paths = list(uploaded_files.keys())\n",
        "print(\"Uploaded PDFs:\", pdf_paths)\n"
      ],
      "metadata": {
        "id": "Qqet9UOfIRpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Extract text (PDF + OCR)\n",
        "\n",
        "from PyPDF2 import PdfReader\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "\n",
        "def extract_text_from_pdf(file_path, use_ocr=True):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        reader = PdfReader(file_path)\n",
        "        for page in reader.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + \"\\n\"\n",
        "    except:\n",
        "        pass\n",
        "    if use_ocr or len(text.strip()) == 0:\n",
        "        images = convert_from_path(file_path)\n",
        "        for img in images:\n",
        "            text += pytesseract.image_to_string(img) + \"\\n\"\n",
        "    return text\n",
        "\n",
        "all_texts = [extract_text_from_pdf(p) for p in pdf_paths]"
      ],
      "metadata": {
        "id": "OcyTgzqgIRsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Chunk text\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks = []\n",
        "for t in all_texts:\n",
        "    chunks.extend(text_splitter.split_text(t))\n",
        "print(f\"Total chunks created: {len(chunks)}\")"
      ],
      "metadata": {
        "id": "z7eoFE2fIdFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Create embeddings + FAISS index\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = embed_model.encode(chunks, convert_to_numpy=True)\n",
        "\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)\n",
        "print(f\"FAISS index contains {index.ntotal} vectors\")"
      ],
      "metadata": {
        "id": "xJX5xaHLIhBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6 - Retrieval function\n",
        "\n",
        "def retrieve(query, index, chunks, top_k=3):\n",
        "    query_embedding = embed_model.encode([query])\n",
        "    D, I = index.search(query_embedding, top_k)\n",
        "    return [chunks[i] for i in I[0]]\n"
      ],
      "metadata": {
        "id": "ptSwTPM9Im3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "adapter_name = \"/content/llama2-7b-qlora-adapter\""
      ],
      "metadata": {
        "id": "x3PqPR1wIr-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load base model with token\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    token=hf_token\n",
        ")"
      ],
      "metadata": {
        "id": "8mmCdTlmItM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load LoRA adapter if exists\n",
        "try:\n",
        "    model = PeftModel.from_pretrained(base, adapter_name)\n",
        "    print(\"LoRA adapter loaded successfully!\")\n",
        "except:\n",
        "    print(\"Adapter not found, using base model.\")\n",
        "    model = base\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, token=hf_token)\n",
        "model.config.use_cache = True"
      ],
      "metadata": {
        "id": "uzdJenEIIydB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Define RAG query function\n",
        "\n",
        "def answer_query(query, top_k=3, max_new_tokens=300):\n",
        "    retrieved_chunks = retrieve(query, index, chunks, top_k=top_k)\n",
        "    context = \"\\n\".join(retrieved_chunks)\n",
        "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "aHUiFg_2I2hN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Run queries\n",
        "\n",
        "query1 = \"Summarize the PDF.\"\n",
        "print(answer_query(query1))\n",
        "\n",
        "query2 = \"Explain the main topics in the PDF.\"\n",
        "print(answer_query(query2))\n",
        "\n",
        "query3 = \"Write a few example questions from this PDF.\"\n",
        "print(answer_query(query3))"
      ],
      "metadata": {
        "id": "gfWV2O_hI6ae"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}